# -*- coding: utf-8 -*-
"""TITANIC SURVIVAL PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0NYiA8pYfdc5uY4VdbfJaI2Lg9giHqJ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from warnings import filterwarnings
filterwarnings(action='ignore')
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier

# Load the dataset
train= pd.read_csv('/content/train.csv')

# Exploratory Data Analysis (EDA)
print("Shape of dataset:",train.shape)

print("Columns in dataset:", train.columns)

print(train.head())

#Checking for Null values
train.isnull().sum()

#Description of dataset
train.describe(include="all")

train.groupby('Survived').mean()

train.corr()

# Exploring relationships
print("\n=== Exploring Relationships ===")
plt.figure(figsize=(12, 8))
sns.heatmap(train.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

sns.pairplot(data=train, hue='Survived')
plt.title('Pairplot of Numerical Features')
plt.show()

male_ind = len(train[train['Sex'] == 'male'])
print("No of Males in Titanic:",male_ind)

female_ind = len(train[train['Sex'] == 'female'])
print("No of Females in Titanic:",female_ind)

# Data
gender = ['Male', 'Female']
index = [577, 314]
# Customize colors
colors = ['steelblue', 'lightcoral']
# Create the figure and axes
fig, ax = plt.subplots(figsize=(6, 4))
# Create the bar plot with customized colors
bars = ax.bar(gender, index, color=colors)
# Adding data labels on top of the bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 10, round(yval), ha='center', color='black', fontweight='bold')
plt.xlabel("Gender")
plt.ylabel("Number of People Onboarding Ship")
plt.title("Onboarding Ship by Gender")
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.show()

alive = len(train[train['Survived'] == 1])
dead = len(train[train['Survived'] == 0])

train.groupby('Sex')[['Survived']].mean()

status = ['Survived', 'Dead']
ind = [alive, dead]  # Replace 'alive' and 'dead' with the actual values
# Customize colors
colors = ['limegreen', 'lightcoral']
# Create the figure and axes
fig, ax = plt.subplots(figsize=(6, 4))

bars = ax.bar(status, ind, color=colors)

# Adding data labels on top of the bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 10, round(yval), ha='center', color='black', fontweight='bold')

# Customize labels and title
plt.xlabel("Status")
plt.ylabel("Number of People")
plt.title("Survival Status")
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.show()

# Create a figure with subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
survived_colors = ['limegreen', 'gold', 'lightcoral']
not_survived_colors = ['lightcoral', 'gold', 'limegreen']

# Plot for survived passengers
train.loc[train['Survived'] == 1, 'Pclass'].value_counts().sort_index().plot.bar(ax=ax1, color=survived_colors)
ax1.set_title('Survived: Ticket Class Distribution')
ax1.set_xlabel('Ticket Class')
ax1.set_ylabel('Number of People')

# Plot for passengers who couldn't survive
train.loc[train['Survived'] == 0, 'Pclass'].value_counts().sort_index().plot.bar(ax=ax2, color=not_survived_colors)
ax2.set_title('Not Survived: Ticket Class Distribution')
ax2.set_xlabel('Ticket Class')
ax2.set_ylabel('Number of People')
plt.tight_layout()
plt.show()

train[["SibSp", "Survived"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)

train[["Pclass", "Survived"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)

train[["Age", "Survived"]].groupby(['Age'], as_index=False).mean().sort_values(by='Age', ascending=True)

train[["Embarked", "Survived"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)

#Droping Useless Columns
train = train.drop(['Ticket'], axis = 1)
train = train.drop(['Cabin'], axis = 1)
train = train.drop(['Name'], axis = 1)

test = pd.read_csv('/content/test.csv')

#Display shape
test.shape

test.isnull().sum()

test.describe(include="all")

test = test.drop(['Ticket'], axis = 1)
test = test.drop(['Cabin'], axis = 1)
test = test.drop(['Name'], axis = 1)

#Feature Selection
column_train=['Age','Pclass','SibSp','Parch','Fare','Sex','Embarked']
#training values
X=train[column_train]
#target value
Y=train['Survived']

X['Age'].isnull().sum()
X['Pclass'].isnull().sum()
X['SibSp'].isnull().sum()
X['Parch'].isnull().sum()
X['Fare'].isnull().sum()
X['Sex'].isnull().sum()
X['Embarked'].isnull().sum()

X['Age']=X['Age'].fillna(X['Age'].median())
X['Age'].isnull().sum()

X['Embarked'] = train['Embarked'].fillna(method ='pad')
X['Embarked'].isnull().sum()

d={'male':0, 'female':1}
X['Sex']=X['Sex'].apply(lambda x:d[x])
X['Sex'].head()

e={'C':0, 'Q':1 ,'S':2}
X['Embarked']=X['Embarked'].apply(lambda x:e[x])
X['Embarked'].head()

#Training Testing and Spliting the model
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,random_state=7)

# Model 1: Random Forest
print("\n=== Random Forest Model ===")
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, Y_train)
rf_predictions = rf_model.predict(X_test)
rf_accuracy = accuracy_score(Y_test, rf_predictions)
rf_report = classification_report(Y_test, rf_predictions)
print(f"Random Forest Accuracy: {rf_accuracy:.2f}")
print("\nClassification Report:\n", rf_report)

# Model 2: Logistic Regression
print("\n=== Logistic Regression Model ===")
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, Y_train)
lr_predictions = lr_model.predict(X_test)
lr_accuracy = accuracy_score(Y_test, lr_predictions)
lr_report = classification_report(Y_test, lr_predictions)
print(f"Logistic Regression Accuracy: {lr_accuracy:.2f}")
print("\nClassification Report:\n", lr_report)

# Model 3: Gradient Boosting
print("\n=== Gradient Boosting Model ===")
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, Y_train)
gb_predictions = gb_model.predict(X_test)
gb_accuracy = accuracy_score(Y_test, gb_predictions)
gb_report = classification_report(Y_test, gb_predictions)
print(f"Gradient Boosting Accuracy: {gb_accuracy:.2f}")
print("\nClassification Report:\n", gb_report)

# Model 4: Support Vector Machine (SVM)
print("\n=== Support Vector Machine (SVM) Model ===")
svm_model = SVC(random_state=42)
svm_model.fit(X_train, Y_train)
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(Y_test, svm_predictions)
svm_report = classification_report(Y_test, svm_predictions)
print(f"SVM Accuracy: {svm_accuracy:.2f}")
print("\nClassification Report:\n", svm_report)

# Model 5: Decision Tree
print("\n=== Decision Tree Model ===")
dt_model= DecisionTreeClassifier(criterion='entropy',random_state=7)
dt_model.fit(X_train,Y_train)
dt_predictions =dt_model.predict(X_test)
dt_accuracy = accuracy_score(Y_test, dt_predictions)
dt_report = classification_report(Y_test, dt_predictions)
print(f"Data Tree Accuracy: {dt_accuracy:.2f}")
print("\nClassification Report:\n", dt_report)

# Model 6: GaussianNB
print("\n=== Naive Bayes Model ===")
nb_model = GaussianNB()
nb_model.fit(X_train,Y_train)
nb_predictions= nb_model.predict(X_test)
nb_accuracy = accuracy_score(Y_test, nb_predictions)
nb_report = classification_report(Y_test, nb_predictions)
print(f" Naive Bayes Accuracy: {dt_accuracy:.2f}")
print("\nClassification Report:\n",dt_report)

final_accuracy = pd.DataFrame({
    'Model': ['Random Forest','Logistic Regression','Gradient Boosting','Support Vector Machines','Decision Tree','Naive Bayes'],
    'Score': [rf_accuracy,lr_accuracy,gb_accuracy,svm_accuracy,dt_accuracy,nb_accuracy]})

final_accuracy_df = final_accuracy.sort_values(by='Score', ascending=False)
final_accuracy_df =final_accuracy_df.set_index('Score')
print(final_accuracy)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.barh(final_accuracy['Model'], final_accuracy['Score'], color='steelblue')

# Adding data labels on the bars
for i, v in enumerate(final_accuracy['Score']):
    plt.text(v + 0.005, i, f'{v:.4f}', va='center', color='black', fontweight='bold')

# Customize labels and title
plt.xlabel('Score')
plt.ylabel('Model')
plt.title('Model Accuracy Comparison')
plt.tight_layout()
plt.show()